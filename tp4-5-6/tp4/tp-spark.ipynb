{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Python\n",
    "\n",
    "PySpark est une interface pour Apache Spark en Python, permettant de traiter de grandes quantités de données en parallèle sur des clusters, en combinant la puissance de calcul de Spark avec la simplicité de Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Déploiement\n",
    "\n",
    "Le déploiement se fait comme pour les TP précédents, à l'aide de Docker Compose. N'oubliez pas de lancer Docker Desktop en premier lieu !\n",
    "```bash \n",
    "docker compose up --build\n",
    "```\n",
    "\n",
    "Sont déployés :\n",
    "- un master Spark \n",
    "- un worker Spark\n",
    "- une installation de [Minio](https://min.io/) servant au stockage des données accédée par Spark\n",
    "- une installation de Jupyter comprenant les bibliothèques nécessaires pour le TP\n",
    "\n",
    "L'UI de Spark est disponible à l'adresse suivante : http://localhost:8080.\n",
    "Nous utilisons la solution de stockage objet Minio, accessible à l'adresse suivante : http://localhost:19001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problèmes possibles\n",
    "\n",
    "Quelques messages d'erreurs que vous pouvez rencontrer, et comment les gérer :\n",
    "- Message d'erreur \"Cannot run multiple SparkContexts at once\" : vous ne pouvez initialiser la connexion avec Spark qu'une fois par notebook, la solution est simplement de faire reset du notebook (bouton restart en haut du notebook).\n",
    "- Plus d'exécuteur disponible dans Spark : un job est problablement déjà en cours. Coupez le sur l'[interface de Spark](http://127.0.0.1:8080) (ou coupez l'ensemble de l'installation avec `docker compose down`), puis faites un reset du notebook (bouton restart en haut du notebook).\n",
    "- L'option restart est grisée : fermez l'onglet du notebook et ouvrez-le à nouveau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-98dba97e-2a83-42e1-9a12-e12c35a0eb74;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.3.4!hadoop-aws.jar (1076ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.12.262!aws-java-sdk-bundle.jar (509925ms)\n",
      "downloading https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.0.7.Final/wildfly-openssl-1.0.7.Final.jar ...\n",
      "\t[SUCCESSFUL ] org.wildfly.openssl#wildfly-openssl;1.0.7.Final!wildfly-openssl.jar (694ms)\n",
      ":: resolution report :: resolve 12453ms :: artifacts dl 511759ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-98dba97e-2a83-42e1-9a12-e12c35a0eb74\n",
      "\tconfs: [default]\n",
      "\t3 artifacts copied, 0 already retrieved (275421kB/7622ms)\n",
      "25/02/25 09:28:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf() \\\n",
    "    .setAppName('SparkApp') \\\n",
    "    .setMaster('spark://spark:7077') \\\n",
    "    .set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") # utilisé pour le stockage \n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark - RDD style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici quelques exemples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of numbers from 1 to 1000 is: 999\n"
     ]
    }
   ],
   "source": [
    "# Create an RDD containing numbers from 1 to 1000\n",
    "numbers_rdd = sc.parallelize(range(1, 1000))\n",
    "\n",
    "# Count the elements in the RDD\n",
    "count = numbers_rdd.count()\n",
    "print(f\"Count of numbers from 1 to 1000 is: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul de moyenne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques liens utiles pour comprendre :\n",
    "- la fonction de création d'un RDD `parallelize` : https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.parallelize.html \n",
    "- la transformation `map` : https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.map.html\n",
    "- la transformation `reduceByKey` : https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduceByKey.html\n",
    "- la fonction `mapValues` : https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.mapValues.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array( ('Brooke', 22.5), ('Denny', 31.0), ('Jules', 30.0), ('TD', 35.0) )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create initial data in Python\n",
    "data = [(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30), (\"TD\", 35), (\"Brooke\", 25)]\n",
    "\n",
    "# Create initial RDD from standard data\n",
    "dataRDD = sc.parallelize(data)\n",
    "\n",
    "# Filter out None values and log data\n",
    "mapped_ages = dataRDD.map(lambda p: (str(p[0]), (int(p[1]), 1)))\n",
    "\n",
    "# Reduce by key to sum ages and count per name\n",
    "summed_ages = mapped_ages.reduceByKey(lambda p1, p2: (p1[0] + p2[0], p1[1] + p2[1]))\n",
    "\n",
    "# Compute the average age per name\n",
    "average_ages = summed_ages.mapValues(lambda v: v[0] / v[1])\n",
    "\n",
    "# Collect the results\n",
    "ages = average_ages.collect()\n",
    "print(\"Array(\", \", \".join([str(age) for age in ages]), \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pagerank\n",
    "\n",
    "PageRank est un algorithme développé par Google pour mesurer l'importance relative de chaque page web en fonction de son nombre et de la qualité des liens entrants. Il attribue un score de popularité à chaque page, influençant son classement dans les résultats de recherche.\n",
    "\n",
    "Quelques liens en plus pour comprendre :\n",
    "- la transformation `join` sur les RDD : https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.join.html\n",
    "- la transformation `flatmap` sur les RDD : https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.flatMap.html\n",
    "\n",
    "N'hésitez pas à essayer de décomposer les calculs pour comprendre ! Attention à bien faire une action de type `collect` pour voir les résultats intermédiaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join [('google.com', (['facebook.com', 'linkedin.com', 'youtube.com'], 0.2)), ('linkedin.com', (['google.com', 'twitter.com'], 0.2)), ('facebook.com', (['google.com', 'twitter.com'], 0.2)), ('twitter.com', (['google.com', 'youtube.com', 'linkedin.com'], 0.2)), ('youtube.com', (['google.com', 'twitter.com'], 0.2))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contribs [('facebook.com', 0.06666666666666667), ('linkedin.com', 0.06666666666666667), ('youtube.com', 0.06666666666666667), ('google.com', 0.1), ('twitter.com', 0.1), ('google.com', 0.1), ('twitter.com', 0.1), ('google.com', 0.06666666666666667), ('youtube.com', 0.06666666666666667), ('linkedin.com', 0.06666666666666667), ('google.com', 0.1), ('twitter.com', 0.1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join [('google.com', (['facebook.com', 'linkedin.com', 'youtube.com'], 0.3416666666666667)), ('linkedin.com', (['google.com', 'twitter.com'], 0.1433333333333333)), ('facebook.com', (['google.com', 'twitter.com'], 0.08666666666666667)), ('twitter.com', (['google.com', 'youtube.com', 'linkedin.com'], 0.28500000000000003)), ('youtube.com', (['google.com', 'twitter.com'], 0.1433333333333333))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve RDD 304\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:713)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:757)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:65)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contribs [('facebook.com', 0.11388888888888889), ('linkedin.com', 0.11388888888888889), ('youtube.com', 0.11388888888888889), ('google.com', 0.07166666666666666), ('twitter.com', 0.07166666666666666), ('google.com', 0.043333333333333335), ('twitter.com', 0.043333333333333335), ('google.com', 0.09500000000000001), ('youtube.com', 0.09500000000000001), ('linkedin.com', 0.09500000000000001), ('google.com', 0.07166666666666666), ('twitter.com', 0.07166666666666666)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join [('google.com', (['facebook.com', 'linkedin.com', 'youtube.com'], 0.26941666666666664)), ('linkedin.com', (['google.com', 'twitter.com'], 0.20755555555555555)), ('facebook.com', (['google.com', 'twitter.com'], 0.12680555555555556)), ('twitter.com', (['google.com', 'youtube.com', 'linkedin.com'], 0.18866666666666665)), ('youtube.com', (['google.com', 'twitter.com'], 0.20755555555555555))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contribs [('facebook.com', 0.08980555555555554), ('linkedin.com', 0.08980555555555554), ('youtube.com', 0.08980555555555554), ('google.com', 0.10377777777777777), ('twitter.com', 0.10377777777777777), ('google.com', 0.06340277777777778), ('twitter.com', 0.06340277777777778), ('google.com', 0.06288888888888888), ('youtube.com', 0.06288888888888888), ('linkedin.com', 0.06288888888888888), ('google.com', 0.10377777777777777), ('twitter.com', 0.10377777777777777)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join [('google.com', (['facebook.com', 'linkedin.com', 'youtube.com'], 0.3137701388888888)), ('linkedin.com', (['google.com', 'twitter.com'], 0.15979027777777774)), ('facebook.com', (['google.com', 'twitter.com'], 0.1063347222222222)), ('twitter.com', (['google.com', 'youtube.com', 'linkedin.com'], 0.2603145833333333)), ('youtube.com', (['google.com', 'twitter.com'], 0.15979027777777774))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contribs [('facebook.com', 0.10459004629629627), ('linkedin.com', 0.10459004629629627), ('youtube.com', 0.10459004629629627), ('google.com', 0.07989513888888887), ('twitter.com', 0.07989513888888887), ('google.com', 0.0531673611111111), ('twitter.com', 0.0531673611111111), ('google.com', 0.08677152777777776), ('youtube.com', 0.08677152777777776), ('linkedin.com', 0.08677152777777776), ('google.com', 0.07989513888888887), ('twitter.com', 0.07989513888888887)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join [('google.com', (['facebook.com', 'linkedin.com', 'youtube.com'], 0.2847697916666666)), ('linkedin.com', (['google.com', 'twitter.com'], 0.19265733796296292)), ('facebook.com', (['google.com', 'twitter.com'], 0.11890153935185183)), ('twitter.com', (['google.com', 'youtube.com', 'linkedin.com'], 0.21101399305555552)), ('youtube.com', (['google.com', 'twitter.com'], 0.19265733796296292))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contribs [('facebook.com', 0.09492326388888887), ('linkedin.com', 0.09492326388888887), ('youtube.com', 0.09492326388888887), ('google.com', 0.09632866898148146), ('twitter.com', 0.09632866898148146), ('google.com', 0.05945076967592591), ('twitter.com', 0.05945076967592591), ('google.com', 0.07033799768518517), ('youtube.com', 0.07033799768518517), ('linkedin.com', 0.07033799768518517), ('google.com', 0.09632866898148146), ('twitter.com', 0.09632866898148146)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join [('google.com', (['facebook.com', 'linkedin.com', 'youtube.com'], 0.30407918952546287)), ('linkedin.com', (['google.com', 'twitter.com'], 0.17047207233796294)), ('facebook.com', (['google.com', 'twitter.com'], 0.11068477430555554)), ('twitter.com', (['google.com', 'youtube.com', 'linkedin.com'], 0.24429189149305547)), ('youtube.com', (['google.com', 'twitter.com'], 0.17047207233796294))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contribs [('facebook.com', 0.10135972984182096), ('linkedin.com', 0.10135972984182096), ('youtube.com', 0.10135972984182096), ('google.com', 0.08523603616898147), ('twitter.com', 0.08523603616898147), ('google.com', 0.05534238715277777), ('twitter.com', 0.05534238715277777), ('google.com', 0.08143063049768516), ('youtube.com', 0.08143063049768516), ('linkedin.com', 0.08143063049768516), ('google.com', 0.08523603616898147), ('twitter.com', 0.08523603616898147)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join [('google.com', (['facebook.com', 'linkedin.com', 'youtube.com'], 0.29115832649016193)), ('linkedin.com', (['google.com', 'twitter.com'], 0.18537180628858022)), ('facebook.com', (['google.com', 'twitter.com'], 0.1161557703655478)), ('twitter.com', (['google.com', 'youtube.com', 'linkedin.com'], 0.22194229056712958)), ('youtube.com', (['google.com', 'twitter.com'], 0.18537180628858022))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contribs [('facebook.com', 0.09705277549672064), ('linkedin.com', 0.09705277549672064), ('youtube.com', 0.09705277549672064), ('google.com', 0.09268590314429011), ('twitter.com', 0.09268590314429011), ('google.com', 0.0580778851827739), ('twitter.com', 0.0580778851827739), ('google.com', 0.07398076352237652), ('youtube.com', 0.07398076352237652), ('linkedin.com', 0.07398076352237652), ('google.com', 0.09268590314429011), ('twitter.com', 0.09268590314429011)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join [('google.com', (['facebook.com', 'linkedin.com', 'youtube.com'], 0.2998158867446711)), ('linkedin.com', (['google.com', 'twitter.com'], 0.1753785081662326)), ('facebook.com', (['google.com', 'twitter.com'], 0.11249485917221254)), ('twitter.com', (['google.com', 'youtube.com', 'linkedin.com'], 0.23693223775065098)), ('youtube.com', (['google.com', 'twitter.com'], 0.1753785081662326))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contribs [('facebook.com', 0.09993862891489036), ('linkedin.com', 0.09993862891489036), ('youtube.com', 0.09993862891489036), ('google.com', 0.0876892540831163), ('twitter.com', 0.0876892540831163), ('google.com', 0.05624742958610627), ('twitter.com', 0.05624742958610627), ('google.com', 0.07897741258355033), ('youtube.com', 0.07897741258355033), ('linkedin.com', 0.07897741258355033), ('google.com', 0.0876892540831163), ('twitter.com', 0.0876892540831163)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join [('google.com', (['facebook.com', 'linkedin.com', 'youtube.com'], 0.29401284778550585)), ('linkedin.com', (['google.com', 'twitter.com'], 0.1820786352736746)), ('facebook.com', (['google.com', 'twitter.com'], 0.1149478345776568)), ('twitter.com', (['google.com', 'youtube.com', 'linkedin.com'], 0.22688204708948803)), ('youtube.com', (['google.com', 'twitter.com'], 0.1820786352736746))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contribs [('facebook.com', 0.09800428259516862), ('linkedin.com', 0.09800428259516862), ('youtube.com', 0.09800428259516862), ('google.com', 0.0910393176368373), ('twitter.com', 0.0910393176368373), ('google.com', 0.0574739172888284), ('twitter.com', 0.0574739172888284), ('google.com', 0.07562734902982934), ('youtube.com', 0.07562734902982934), ('linkedin.com', 0.07562734902982934), ('google.com', 0.0910393176368373), ('twitter.com', 0.0910393176368373)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join [('google.com', (['facebook.com', 'linkedin.com', 'youtube.com'], 0.2979029163534824)), ('linkedin.com', (['google.com', 'twitter.com'], 0.17758688688124824)), ('facebook.com', (['google.com', 'twitter.com'], 0.11330364020589333)), ('twitter.com', (['google.com', 'youtube.com', 'linkedin.com'], 0.23361966967812756)), ('youtube.com', (['google.com', 'twitter.com'], 0.17758688688124824))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contribs [('facebook.com', 0.09930097211782747), ('linkedin.com', 0.09930097211782747), ('youtube.com', 0.09930097211782747), ('google.com', 0.08879344344062412), ('twitter.com', 0.08879344344062412), ('google.com', 0.056651820102946664), ('twitter.com', 0.056651820102946664), ('google.com', 0.07787322322604252), ('youtube.com', 0.07787322322604252), ('linkedin.com', 0.07787322322604252), ('google.com', 0.08879344344062412), ('twitter.com', 0.08879344344062412)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 287:==========================================>              (6 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('google.com', 0.2952951406787018), ('linkedin.com', 0.18059806604228948), ('facebook.com', 0.11440582630015335), ('twitter.com', 0.22910290093656566), ('youtube.com', 0.18059806604228948)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Parameters\n",
    "ITERATIONS, a, N = 10, 0.15, 5\n",
    "\n",
    "# Inline example data: (Website, List[Linked Websites])\n",
    "data = [\n",
    "    (\"google.com\", [\"facebook.com\", \"linkedin.com\", \"youtube.com\"]),\n",
    "    (\"twitter.com\", [\"google.com\", \"youtube.com\", \"linkedin.com\"]),\n",
    "    (\"facebook.com\", [\"google.com\", \"twitter.com\"]),\n",
    "    (\"youtube.com\", [\"google.com\", \"twitter.com\"]),\n",
    "    (\"linkedin.com\", [\"google.com\", \"twitter.com\"])\n",
    "]\n",
    "\n",
    "links = sc.parallelize(data).partitionBy(8).persist()\n",
    "\n",
    "# Initialize ranks: RDD of (Website, initial rank)\n",
    "ranks = links.mapValues(lambda _: 1.0 / N)\n",
    "\n",
    "# PageRank Iterations\n",
    "for _ in range(ITERATIONS):\n",
    "    print(\"join\", links.join(ranks).collect())\n",
    "    contribs = links.join(ranks).flatMap(\n",
    "        lambda site_links_rank: [(dest, site_links_rank[1][1] / len(site_links_rank[1][0])) \n",
    "                                 for dest in site_links_rank[1][0]]\n",
    "    )\n",
    "    print(\"contribs\", contribs.collect())\n",
    "    ranks = contribs.reduceByKey(lambda x, y: x + y).mapValues(lambda total: a / N + (1 - a) * total)\n",
    "\n",
    "# Output final ranks\n",
    "print(ranks.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark + Minio\n",
    "\n",
    "Nous allons utiliser le stockage objet Minio pour héberger les données de notre installation Spark.\n",
    "\n",
    "MinIO est une solution de stockage objet haute performance, compatible avec l'API S3 d'AWS, permettant de gérer des données non structurées à grande échelle. Il est conçu pour des environnements cloud, hybrides ou sur site, offrant une infrastructure de stockage distribuée et évolutive.\n",
    "\n",
    "La copie de fichier peut se faire par la bibliothèque Minio Python, ou alors par le biais de l'UI Web, accessible sur http://localhost:19001. Les identifiants sont \"root\" et \"password\" (on peut les retrouver dans le fichier [docker-compose.yml](docker-compose.yml)). \n",
    "\n",
    "Le principe général de MinIO (comme pour les autres systèmes de stockage objet tels qu'AWS S3) est d'organiser les données en buckets, qui sont des conteneurs virtuels pour le stockage de fichiers ou d’objets. Chaque bucket est unique dans le système et peut contenir un nombre illimité d'objets, identifiés par des clés uniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice: Moby Dick\n",
    "\n",
    "Prérequis : le contenu du livre est présent dans le fichier `pg2701.txt` ([lien internet](https://nyu-cds.github.io/python-bigdata/files/pg2701.txt)) du TP4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-25 10:44:44--  https://nyu-cds.github.io/python-bigdata/files/pg2701.txt\n",
      "Resolving nyu-cds.github.io (nyu-cds.github.io)... 185.199.111.153, 185.199.110.153, 185.199.108.153, ...\n",
      "Connecting to nyu-cds.github.io (nyu-cds.github.io)|185.199.111.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1257296 (1.2M) [text/plain]\n",
      "Saving to: ‘pg2701.txt’\n",
      "\n",
      "pg2701.txt          100%[===================>]   1.20M  3.71MB/s    in 0.3s    \n",
      "\n",
      "2025-02-25 10:44:47 (3.71 MB/s) - ‘pg2701.txt’ saved [1257296/1257296]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://nyu-cds.github.io/python-bigdata/files/pg2701.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramètres utilisés pour stockage\n",
    "import docker\n",
    "\n",
    "minio_ip_address = \"minio\"\n",
    "\n",
    "# Décommenter dans le cas de WSL \n",
    "#network_name = \"tp4_default\"\n",
    "#container_name = \"minio\"\n",
    "#\n",
    "#client = docker.from_env()\n",
    "#container = client.containers.get(container_name)\n",
    "#\n",
    "#minio_ip_address: str = container.attrs['NetworkSettings']['Networks'][network_name]['IPAddress']\n",
    "#minio_ip_address\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", f\"http://{minio_ip_address}:9000\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"root\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"password\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minio.helpers.ObjectWriteResult at 0x7f6c96a22ae0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://min.io/docs/minio/linux/developers/python/API.html\n",
    "\n",
    "from minio import Minio\n",
    "client_minio = Minio(\n",
    "    f\"{minio_ip_address}:9000\",\n",
    "    access_key=\"root\",\n",
    "    secret_key=\"password\",\n",
    "    secure=False\n",
    ")\n",
    "\n",
    "# Création du bucket tp5\n",
    "if client_minio.bucket_exists(\"tp4\") == False:\n",
    "    client_minio.make_bucket(\"tp4\")\n",
    "client_minio.fput_object(\"tp4\", \"pg2701.txt\", \"pg2701.txt\") # copie du fichier local dans le bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minio reprend le principe du stockage cloud S3 : il permet de stocker des fichiers dans des \"buckets\". Les buckets dans MinIO sont des conteneurs de stockage pour organiser et gérer des objets (fichiers) de manière structurée, similaires aux dossiers dans un système de fichiers, mais optimisés pour le stockage objet.\n",
    "Vérifiez que le fichier est bien présent dans le bucket `tp4` de Minio : [http://localhost:19001/browser/tp4/](http://localhost:19001/browser/tp4/). Vous pouvez utiliser l'utilisateur `root` et le mot de passe `password` pour vous connecter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/25 10:47:06 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "[Stage 288:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Project Gutenberg EBook of Moby Dick; or The Whale, by Herman Melville', '', 'This eBook is for the use of anyone anywhere at no cost and with', 'almost no restrictions whatsoever.  You may copy it, give it away or', 're-use it under the terms of the Project Gutenberg License included', 'with this eBook or online at www.gutenberg.org', '', '', 'Title: Moby Dick; or The Whale', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "minio_file = \"s3a://tp4/pg2701.txt\"\n",
    "# adresse du fichier dans le bucket minio\n",
    "text = sc.textFile(minio_file) \n",
    "print(text.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1\n",
    "\n",
    "1. Compter le nombre de mots total du livre.\n",
    "2. Compter le nombre d'occurrences par mot, trier par nombre décroissant (prendre les 10 premiers).\n",
    "3. Compter le nombre de mots moyen par phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172256\n",
      "172256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "from operator import add\n",
    "print(text.map(lambda line: len(line.split(\" \"))).reduce(add))\n",
    "print(text.flatMap(lambda line: line.split(\" \")).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 312:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 7905), ('the', 7436), ('and', 5638), ('I', 4495), ('to', 4463), ('of', 3679), ('a', 2887), ('in', 2385), ('that', 2322), ('he', 1948)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "print(text.flatMap(lambda line: line.split(\" \")).map(lambda v: (v, 1)).reduceByKey(add).takeOrdered(10, lambda v: -v[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Project Gutenberg eBook of Dracula      This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever', ' You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this ebook or online at www', 'gutenberg', 'org', ' If you are not located in the United States, you will have to check the laws of the country where you are located before using this eBook', '  Title: Dracula  Author: Bram Stoker  Release date: October 1, 1995 [eBook #345]                 Most recently updated: November 12, 2023  Language: English  Credits: Chuck Greif and the Online Distributed Proofreading Team   *** START OF THE PROJECT GUTENBERG EBOOK DRACULA ***                                     DRACULA                                    _by_                                Bram Stoker                          [Illustration: colophon]                                  NEW YORK                              GROSSET & DUNLAP                                _Publishers_        Copyright, 1897, in the United States of America, according                    to Act of Congress, by Bram Stoker                          [_All rights reserved', '_]                        PRINTED IN THE UNITED STATES                                    AT                THE COUNTRY LIFE PRESS, GARDEN CITY, N', 'Y', '                                        TO                               MY DEAR FRIEND                                 HOMMY-BEG     Contents  CHAPTER I', ' Jonathan Harker’s Journal CHAPTER II']\n",
      "21.18219097832457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "textBis = sc.parallelize(text.reduce(lambda x, y: \" \".join([x, y])).split(\".\"))\n",
    "print(textBis.take(10))\n",
    "print(textBis.map(lambda p: len(p.split(\" \"))).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Télécharger un autre livre (en trouver un sur https://www.gutenberg.org/browse/scores/top par exemple, télécharger au format \"Plain Text UTF-8\"), et lancer les jobs dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-25 11:17:45--  https://www.gutenberg.org/cache/epub/345/pg345.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 890394 (870K) [text/plain]\n",
      "Saving to: ‘pg345.txt’\n",
      "\n",
      "pg345.txt           100%[===================>] 869.53K  1.15MB/s    in 0.7s    \n",
      "\n",
      "2025-02-25 11:17:47 (1.15 MB/s) - ‘pg345.txt’ saved [890394/890394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.gutenberg.org/cache/epub/345/pg345.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minio.helpers.ObjectWriteResult at 0x7f6c9427c5c0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fput client minio\n",
    "client_minio.fput_object(\"tp4\", \"pg345.txt\", \"pg345.txt\") # copie du fichier local dans le bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 309:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Project Gutenberg eBook of Dracula', '    ', 'This ebook is for the use of anyone anywhere in the United States and', 'most other parts of the world at no cost and with almost no restrictions', 'whatsoever. You may copy it, give it away or re-use it under the terms', 'of the Project Gutenberg License included with this ebook or online', 'at www.gutenberg.org. If you are not located in the United States,', 'you will have to check the laws of the country where you are located', 'before using this eBook.', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#traitements\n",
    "minio_file = \"s3a://tp4/pg345.txt\"\n",
    "# adresse du fichier dans le bucket minio\n",
    "text = sc.textFile(minio_file) \n",
    "print(text.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2 : JSON\n",
    "\n",
    "Charger le fichier `countries.json` ((adresse)[http://api.worldbank.org/v2/countries?per_page=304&format=json]), et chargez le dans Minio comme fait précédemment. \n",
    "\n",
    "Calculez ensuite le nombre de pays par niveau de revenu à l'aide d'un job Pyspark.\n",
    "\n",
    "Vous pouvez vous aider des lignes suivantes en premier lieu (afin de se concentrer directement sur le tableau des pays, contenu dans le deuxième élément du tableau racine) :\n",
    "```python\n",
    "rdd = sc.textFile(f\"s3a://tp4/countries.json\") # chargement du fichier countries.json\n",
    "mapped_rdd = rdd.map(lambda f: json.loads(f)) # chargement du fichier json dans un dictionnaire\n",
    "country_rdd = mapped_rdd.flatMap(lambda x: x[1]) # on récupère le tableau des pays\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-25 11:20:41--  http://api.worldbank.org/v2/countries?per_page=304&format=json\n",
      "Resolving api.worldbank.org (api.worldbank.org)... 172.64.145.25, 104.18.42.231, 2606:4700:4400::6812:2ae7, ...\n",
      "Connecting to api.worldbank.org (api.worldbank.org)|172.64.145.25|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘countries.json’\n",
      "\n",
      "countries.json          [ <=>                ] 110.21K  --.-KB/s    in 0.003s  \n",
      "\n",
      "2025-02-25 11:20:41 (38.0 MB/s) - ‘countries.json’ saved [112851]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O countries.json  \"http://api.worldbank.org/v2/countries?per_page=304&format=json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minio.helpers.ObjectWriteResult at 0x7f6c94422060>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fput client minio\n",
    "client_minio.fput_object(\"tp4\", \"countries.json\", \"countries.json\") # copie du fichier local dans le bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/25 15:45:07 WARN HeartbeatReceiver: Removing executor 0 with no recent heartbeats: 15530334 ms exceeds timeout 120000 ms\n",
      "25/02/25 15:45:09 ERROR TaskSchedulerImpl: Lost executor 0 on 172.24.0.2: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/02/25 15:45:11 ERROR Inbox: Ignoring error\n",
      "java.lang.AssertionError: assertion failed: BlockManager re-registration shouldn't succeed when the executor is lost\n",
      "\tat scala.Predef$.assert(Predef.scala:223)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:727)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/02/25 15:45:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_309_6 !\n",
      "25/02/25 15:45:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_309_7 !\n",
      "25/02/25 15:45:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_309_1 !\n",
      "25/02/25 15:45:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_309_5 !\n",
      "25/02/25 15:45:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_309_4 !\n",
      "25/02/25 15:45:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_309_0 !\n",
      "25/02/25 15:45:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_309_2 !\n",
      "25/02/25 15:45:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_309_3 !\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.textFile(f\"s3a://tp4/countries.json\") # chargement du fichier countries.json\n",
    "mapped_rdd = rdd.map(lambda f: json.loads(f)) # chargement du fichier json dans un dictionnaire\n",
    "country_rdd = mapped_rdd.flatMap(lambda x: x[1]) # on récupère le tableau des pays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de pays par niveau de revenu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 3\n",
    "\n",
    "Réalisez trois requêtes non triviales sur les fichiers json des [pokemon](https://github.com/fanzeyi/pokemon.json).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
