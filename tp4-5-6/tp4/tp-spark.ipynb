{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Python\n",
    "\n",
    "PySpark est une interface pour Apache Spark en Python, permettant de traiter de grandes quantités de données en parallèle sur des clusters, en combinant la puissance de calcul de Spark avec la simplicité de Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Déploiement\n",
    "\n",
    "Le déploiement se fait comme pour les TP précédents, à l'aide de Docker Compose. N'oubliez pas de lancer Docker Desktop en premier lieu !\n",
    "```bash \n",
    "docker compose -f docker-compose-jupyter.yml up\n",
    "```\n",
    "\n",
    "Sont déployés :\n",
    "- un master Spark \n",
    "- un worker Spark\n",
    "- une installation de [Minio](https://min.io/) servant au stockage des données accédée par Spark\n",
    "- une installation de Jupyter comprenant les bibliothèques nécessaires pour le TP\n",
    "\n",
    "L'UI de Spark est disponible à l'adresse suivante : http://localhost:8080.\n",
    "Nous utilisons la solution de stockage objet Minio, accessible à l'adresse suivante : http://localhost:19001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problèmes possibles\n",
    "\n",
    "Quelques messages d'erreurs que vous pouvez rencontrer, et comment les gérer :\n",
    "- Message d'erreur \"Cannot run multiple SparkContexts at once\" : vous ne pouvez initialiser la connexion avec Spark qu'une fois par notebook, la solution est simplement de faire reset du notebook (bouton restart en haut du notebook).\n",
    "- Plus d'exécuteur disponible dans Spark : un job est problablement déjà en cours. Coupez le sur l'[interface de Spark](http://127.0.0.1:8080) (ou coupez l'ensemble de l'installation avec `docker compose down`), puis faites un reset du notebook (bouton restart en haut du notebook).\n",
    "- L'option restart est grisée : fermez l'onglet du notebook et ouvrez-le à nouveau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-68edb3ac-1432-4416-bb2f-aa4505121ff8;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.3.4!hadoop-aws.jar (80ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.12.262!aws-java-sdk-bundle.jar (3065ms)\n",
      "downloading https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.0.7.Final/wildfly-openssl-1.0.7.Final.jar ...\n",
      "\t[SUCCESSFUL ] org.wildfly.openssl#wildfly-openssl;1.0.7.Final!wildfly-openssl.jar (43ms)\n",
      ":: resolution report :: resolve 4041ms :: artifacts dl 3192ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-68edb3ac-1432-4416-bb2f-aa4505121ff8\n",
      "\tconfs: [default]\n",
      "\t3 artifacts copied, 0 already retrieved (275421kB/255ms)\n",
      "25/02/18 12:36:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 58898)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.12/socketserver.py\", line 318, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.12/socketserver.py\", line 349, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.12/socketserver.py\", line 362, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.12/socketserver.py\", line 761, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf() \\\n",
    "    .setAppName('SparkApp') \\\n",
    "    .setMaster('spark://spark:7077') \\\n",
    "    .set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") # utilisé pour le stockage \n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pagerank\n",
    "\n",
    "PageRank est un algorithme développé par Google pour mesurer l'importance relative de chaque page web en fonction de son nombre et de la qualité des liens entrants. Il attribue un score de popularité à chaque page, influençant son classement dans les résultats de recherche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('google.com', 0.2952951406787018), ('linkedin.com', 0.18059806604228948), ('facebook.com', 0.11440582630015335), ('twitter.com', 0.22910290093656566), ('youtube.com', 0.18059806604228948)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Parameters\n",
    "ITERATIONS, a, N = 10, 0.15, 5\n",
    "\n",
    "# Inline example data: (Website, List[Linked Websites])\n",
    "data = [\n",
    "    (\"google.com\", [\"facebook.com\", \"linkedin.com\", \"youtube.com\"]),\n",
    "    (\"twitter.com\", [\"google.com\", \"youtube.com\", \"linkedin.com\"]),\n",
    "    (\"facebook.com\", [\"google.com\", \"twitter.com\"]),\n",
    "    (\"youtube.com\", [\"google.com\", \"twitter.com\"]),\n",
    "    (\"linkedin.com\", [\"google.com\", \"twitter.com\"])\n",
    "]\n",
    "\n",
    "links = sc.parallelize(data).partitionBy(8).persist()\n",
    "\n",
    "# Initialize ranks: RDD of (Website, initial rank)\n",
    "ranks = links.mapValues(lambda _: 1.0 / N)\n",
    "\n",
    "# PageRank Iterations\n",
    "for _ in range(ITERATIONS):\n",
    "    contribs = links.join(ranks).flatMap(\n",
    "        lambda site_links_rank: [(dest, site_links_rank[1][1] / len(site_links_rank[1][0])) \n",
    "                                 for dest in site_links_rank[1][0]]\n",
    "    )\n",
    "    ranks = contribs.reduceByKey(lambda x, y: x + y).mapValues(lambda total: a / N + (1 - a) * total)\n",
    "\n",
    "# Output final ranks\n",
    "print(ranks.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark - RDD style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici quelques exemples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of numbers from 1 to 1000 is: 999\n"
     ]
    }
   ],
   "source": [
    "# Create an RDD containing numbers from 1 to 1000\n",
    "numbers_rdd = sc.parallelize(range(1, 1000))\n",
    "\n",
    "# Count the elements in the RDD\n",
    "count = numbers_rdd.count()\n",
    "print(f\"Count of numbers from 1 to 1000 is: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul de moyenne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array( ('Brooke', 22.5), ('Denny', 31.0), ('Jules', 30.0), ('TD', 35.0) )\n"
     ]
    }
   ],
   "source": [
    "# Create initial data in Python\n",
    "data = [(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30), (\"TD\", 35), (\"Brooke\", 25)]\n",
    "\n",
    "# Create initial RDD from standard data\n",
    "dataRDD = sc.parallelize(data)\n",
    "\n",
    "# Safe mapping function with type checks\n",
    "def safe_map(p):\n",
    "    return (str(p[0]), (int(p[1]), 1))\n",
    "\n",
    "# Filter out None values and log data\n",
    "mapped_ages = dataRDD.map(lambda p: (str(p[0]), (int(p[1]), 1)))\n",
    "# Alternative : safe_map\n",
    "# mapped_ages = dataRDD.map(safe_map)\n",
    "\n",
    "# Reduce by key to sum ages and count per name\n",
    "summed_ages = mapped_ages.reduceByKey(lambda p1, p2: (p1[0] + p2[0], p1[1] + p2[1]))\n",
    "\n",
    "# Compute the average age per name\n",
    "average_ages = summed_ages.mapValues(lambda v: v[0] / v[1])\n",
    "\n",
    "# Collect the results\n",
    "ages = average_ages.collect()\n",
    "print(\"Array(\", \", \".join([str(age) for age in ages]), \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark + Minio\n",
    "\n",
    "Nous allons utiliser le stockage objet Minio pour héberger les données de notre installation Spark.\n",
    "\n",
    "MinIO est une solution de stockage objet haute performance, compatible avec l'API S3 d'AWS, permettant de gérer des données non structurées à grande échelle. Il est conçu pour des environnements cloud, hybrides ou sur site, offrant une infrastructure de stockage distribuée et évolutive.\n",
    "\n",
    "La copie de fichier peut se faire par la bibliothèque Minio Python, ou alors par le biais de l'UI Web, accessible sur http://localhost:19001. Les identifiants sont \"root\" et \"password\" (on peut les retrouver dans le fichier [docker-compose.yml](docker-compose.yml)). \n",
    "\n",
    "Le principe général de MinIO (comme pour les autres systèmes de stockage objet tels qu'AWS S3) est d'organiser les données en buckets, qui sont des conteneurs virtuels pour le stockage de fichiers ou d’objets. Chaque bucket est unique dans le système et peut contenir un nombre illimité d'objets, identifiés par des clés uniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice: Moby Dick\n",
    "\n",
    "Prérequis : télécharger le livre à l'[emplacement suivant](https://nyu-cds.github.io/python-bigdata/files/pg2701.txt), et placer le fichier `pg2701.txt` dans le dossier du TP5. Ceci est automatisé par la commande suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1227k  100 1227k    0     0  4409k      0 --:--:-- --:--:-- --:--:-- 4416k\n"
     ]
    }
   ],
   "source": [
    "!curl https://nyu-cds.github.io/python-bigdata/files/pg2701.txt -o pg2701.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramètres utilisés pour stockage\n",
    "import docker\n",
    "\n",
    "minio_ip_address = \"minio\"\n",
    "\n",
    "# Décommenter dans le cas de WSL \n",
    "#network_name = \"tp5_default\"\n",
    "#container_name = \"minio\"\n",
    "#\n",
    "#client = docker.from_env()\n",
    "#container = client.containers.get(container_name)\n",
    "#\n",
    "#minio_ip_address: str = container.attrs['NetworkSettings']['Networks'][network_name]['IPAddress']\n",
    "#minio_ip_address\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", f\"http://{minio_ip_address}:9000\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"root\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"password\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minio.helpers.ObjectWriteResult at 0x781478258fb0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://min.io/docs/minio/linux/developers/python/API.html\n",
    "\n",
    "from minio import Minio\n",
    "client_minio = Minio(\n",
    "    f\"{minio_ip_address}:9000\",\n",
    "    access_key=\"root\",\n",
    "    secret_key=\"password\",\n",
    "    secure=False\n",
    ")\n",
    "\n",
    "# Création du bucket tp5\n",
    "if client_minio.bucket_exists(\"tp5\") == False:\n",
    "    client_minio.make_bucket(\"tp5\")\n",
    "client_minio.fput_object(\"tp5\", \"pg2701.txt\", \"pg2701.txt\") # copie du fichier local dans le bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minio reprend le principe du stockage cloud S3 : il permet de stocker des fichiers dans des \"buckets\". Les buckets dans MinIO sont des conteneurs de stockage pour organiser et gérer des objets (fichiers) de manière structurée, similaires aux dossiers dans un système de fichiers, mais optimisés pour le stockage objet.\n",
    "Vérifiez que le fichier est bien présent dans le bucket `tp5` de Minio : [http://localhost:19001/browser/tp5/](http://localhost:19001/browser/tp5/). Vous pouvez utiliser l'utilisateur `root` et le mot de passe `password` pour vous connecter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/18 13:22:28 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Project Gutenberg EBook of Moby Dick; or The Whale, by Herman Melville', '', 'This eBook is for the use of anyone anywhere at no cost and with', 'almost no restrictions whatsoever.  You may copy it, give it away or', 're-use it under the terms of the Project Gutenberg License included', 'with this eBook or online at www.gutenberg.org', '', '', 'Title: Moby Dick; or The Whale', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "minio_file = \"s3a://tp5/pg2701.txt\"\n",
    "# adresse du fichier dans le bucket minio\n",
    "text = sc.textFile(minio_file) \n",
    "print(text.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1\n",
    "\n",
    "Compter le nombre de mots total du livre.\n",
    "Compter le nombre d'occurrences par mot, trier par nombre décroissant (prendre les 10 premiers).\n",
    "Compter le nombre de mots par phrase.\n",
    "\n",
    "Télécharger un autre livre (en trouver un sur https://www.gutenberg.org/browse/scores/top par exemple, télécharger au format \"Plain Text UTF-8\"), et lancer les jobs dessus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2 : JSON\n",
    "\n",
    "Charger le fichier `countries.json` à l'adresse suivante :  http://api.worldbank.org/v2/countries?per_page=304&format=json, et chargez le dans Minio comme fait précédemment. \n",
    "\n",
    "Calculez ensuite le nombre de pays par niveau de revenu à l'aide d'un job Pyspark.\n",
    "\n",
    "Vous pouvez vous aider des lignes suivantes en premier lieu (afin de se concentrer directement sur le tableau des pays, contenu dans le deuxième élément du tableau racine) :\n",
    "```python\n",
    "rdd = sc.textFile(f\"s3a://tp5/countries.json\") # chargement du fichier countries.json\n",
    "mapped_rdd = rdd.map(lambda f: json.loads(f)) # chargement du fichier json dans un dictionnaire\n",
    "country_rdd = mapped_rdd.flatMap(lambda x: x[1]) # on récupère le tableau des pays\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 3\n",
    "\n",
    "Réalisez trois requêtes non triviales sur un fichier JSON de votre projet du TP4.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
