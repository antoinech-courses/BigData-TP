{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Python\n",
    "\n",
    "PySpark est une interface pour Apache Spark en Python, permettant de traiter de grandes quantités de données en parallèle sur des clusters, en combinant la puissance de calcul de Spark avec la simplicité de Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Déploiement\n",
    "\n",
    "Le déploiement se fait comme pour les TP précédents, à l'aide de Docker Compose. N'oubliez pas de lancer Docker Desktop en premier lieu !\n",
    "```bash \n",
    "docker compose up --build\n",
    "```\n",
    "\n",
    "Sont déployés :\n",
    "- un master Spark \n",
    "- un worker Spark\n",
    "- une installation de [Minio](https://min.io/) servant au stockage des données accédée par Spark\n",
    "- une installation de Jupyter comprenant les bibliothèques nécessaires pour le TP\n",
    "\n",
    "L'UI de Spark est disponible à l'adresse suivante : http://localhost:8080.\n",
    "Nous utilisons la solution de stockage objet Minio, accessible à l'adresse suivante : http://localhost:19001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problèmes possibles\n",
    "\n",
    "Quelques messages d'erreurs que vous pouvez rencontrer, et comment les gérer :\n",
    "- Message d'erreur \"Cannot run multiple SparkContexts at once\" : vous ne pouvez initialiser la connexion avec Spark qu'une fois par notebook, la solution est simplement de faire reset du notebook (bouton restart en haut du notebook).\n",
    "- Plus d'exécuteur disponible dans Spark : un job est problablement déjà en cours. Coupez le sur l'[interface de Spark](http://127.0.0.1:8080) (ou coupez l'ensemble de l'installation avec `docker compose down`), puis faites un reset du notebook (bouton restart en haut du notebook).\n",
    "- L'option restart est grisée : fermez l'onglet du notebook et ouvrez-le à nouveau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-07c9bc79-fcec-47b0-8466-44def05a1067;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.3.4!hadoop-aws.jar (114ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.12.262!aws-java-sdk-bundle.jar (102532ms)\n",
      "downloading https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.0.7.Final/wildfly-openssl-1.0.7.Final.jar ...\n",
      "\t[SUCCESSFUL ] org.wildfly.openssl#wildfly-openssl;1.0.7.Final!wildfly-openssl.jar (219ms)\n",
      ":: resolution report :: resolve 3065ms :: artifacts dl 102877ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-07c9bc79-fcec-47b0-8466-44def05a1067\n",
      "\tconfs: [default]\n",
      "\t3 artifacts copied, 0 already retrieved (275421kB/217ms)\n",
      "25/02/25 09:58:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf() \\\n",
    "    .setAppName('SparkApp') \\\n",
    "    .setMaster('spark://spark:7077') \\\n",
    "    .set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") # utilisé pour le stockage \n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark - RDD style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici quelques exemples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of numbers from 1 to 1000 is: 999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create an RDD containing numbers from 1 to 1000\n",
    "numbers_rdd = sc.parallelize(range(1, 1000))\n",
    "\n",
    "# Count the elements in the RDD\n",
    "count = numbers_rdd.count()\n",
    "print(f\"Count of numbers from 1 to 1000 is: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul de moyenne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques liens utiles pour comprendre :\n",
    "- la fonction de création d'un RDD `parallelize` : https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.parallelize.html \n",
    "- la transformation `map` : https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.map.html\n",
    "- la transformation `reduceByKey` : https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduceByKey.html\n",
    "- la fonction `mapValues` : https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.mapValues.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array( ('Brooke', 22.5), ('Denny', 31.0), ('Jules', 30.0), ('TD', 35.0) )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create initial data in Python\n",
    "data = [(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30), (\"TD\", 35), (\"Brooke\", 25)]\n",
    "\n",
    "# Create initial RDD from standard data\n",
    "dataRDD = sc.parallelize(data)\n",
    "\n",
    "# Filter out None values and log data\n",
    "mapped_ages = dataRDD.map(lambda p: (str(p[0]), (int(p[1]), 1)))\n",
    "\n",
    "# Reduce by key to sum ages and count per name\n",
    "summed_ages = mapped_ages.reduceByKey(lambda p1, p2: (p1[0] + p2[0], p1[1] + p2[1]))\n",
    "\n",
    "# Compute the average age per name\n",
    "average_ages = summed_ages.mapValues(lambda v: v[0] / v[1])\n",
    "\n",
    "# Collect the results\n",
    "ages = average_ages.collect()\n",
    "print(\"Array(\", \", \".join([str(age) for age in ages]), \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pagerank\n",
    "\n",
    "PageRank est un algorithme développé par Google pour mesurer l'importance relative de chaque page web en fonction de son nombre et de la qualité des liens entrants. Il attribue un score de popularité à chaque page, influençant son classement dans les résultats de recherche.\n",
    "\n",
    "Quelques liens en plus pour comprendre :\n",
    "- la transformation `join` sur les RDD : https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.join.html\n",
    "- la transformation `flatmap` sur les RDD : https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.flatMap.html\n",
    "\n",
    "N'hésitez pas à essayer de décomposer les calculs pour comprendre ! Attention à bien faire une action de type `collect` pour voir les résultats intermédiaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('google.com', ['facebook.com', 'linkedin.com', 'youtube.com']), ('linkedin.com', ['google.com', 'twitter.com']), ('facebook.com', ['google.com', 'twitter.com']), ('twitter.com', ['google.com', 'youtube.com', 'linkedin.com']), ('youtube.com', ['google.com', 'twitter.com'])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('google.com', (['facebook.com', 'linkedin.com', 'youtube.com'], 0.2)), ('linkedin.com', (['google.com', 'twitter.com'], 0.2)), ('facebook.com', (['google.com', 'twitter.com'], 0.2)), ('twitter.com', (['google.com', 'youtube.com', 'linkedin.com'], 0.2)), ('youtube.com', (['google.com', 'twitter.com'], 0.2))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('linkedin.com', (['google.com', 'twitter.com'], 0.2))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['google.com', 'twitter.com'], 0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linkedin.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('facebook.com', 0.06666666666666667), ('linkedin.com', 0.06666666666666667), ('youtube.com', 0.06666666666666667), ('google.com', 0.1), ('twitter.com', 0.1), ('google.com', 0.1), ('twitter.com', 0.1), ('google.com', 0.06666666666666667), ('youtube.com', 0.06666666666666667), ('linkedin.com', 0.06666666666666667), ('google.com', 0.1), ('twitter.com', 0.1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('google.com', 0.3416666666666667), ('linkedin.com', 0.1433333333333333), ('facebook.com', 0.08666666666666667), ('twitter.com', 0.28500000000000003), ('youtube.com', 0.1433333333333333)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('facebook.com', 0.11388888888888889), ('linkedin.com', 0.11388888888888889), ('youtube.com', 0.11388888888888889), ('google.com', 0.07166666666666666), ('twitter.com', 0.07166666666666666), ('google.com', 0.043333333333333335), ('twitter.com', 0.043333333333333335), ('google.com', 0.09500000000000001), ('youtube.com', 0.09500000000000001), ('linkedin.com', 0.09500000000000001), ('google.com', 0.07166666666666666), ('twitter.com', 0.07166666666666666)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('google.com', 0.26941666666666664), ('linkedin.com', 0.20755555555555555), ('facebook.com', 0.12680555555555556), ('twitter.com', 0.18866666666666665), ('youtube.com', 0.20755555555555555)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('facebook.com', 0.08980555555555554), ('linkedin.com', 0.08980555555555554), ('youtube.com', 0.08980555555555554), ('google.com', 0.10377777777777777), ('twitter.com', 0.10377777777777777), ('google.com', 0.06340277777777778), ('twitter.com', 0.06340277777777778), ('google.com', 0.06288888888888888), ('youtube.com', 0.06288888888888888), ('linkedin.com', 0.06288888888888888), ('google.com', 0.10377777777777777), ('twitter.com', 0.10377777777777777)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('google.com', 0.3137701388888888), ('linkedin.com', 0.15979027777777774), ('facebook.com', 0.1063347222222222), ('twitter.com', 0.2603145833333333), ('youtube.com', 0.15979027777777774)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('facebook.com', 0.10459004629629627), ('linkedin.com', 0.10459004629629627), ('youtube.com', 0.10459004629629627), ('google.com', 0.07989513888888887), ('twitter.com', 0.07989513888888887), ('google.com', 0.0531673611111111), ('twitter.com', 0.0531673611111111), ('google.com', 0.08677152777777776), ('youtube.com', 0.08677152777777776), ('linkedin.com', 0.08677152777777776), ('google.com', 0.07989513888888887), ('twitter.com', 0.07989513888888887)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('google.com', 0.2847697916666666), ('linkedin.com', 0.19265733796296292), ('facebook.com', 0.11890153935185183), ('twitter.com', 0.21101399305555552), ('youtube.com', 0.19265733796296292)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('facebook.com', 0.09492326388888887), ('linkedin.com', 0.09492326388888887), ('youtube.com', 0.09492326388888887), ('google.com', 0.09632866898148146), ('twitter.com', 0.09632866898148146), ('google.com', 0.05945076967592591), ('twitter.com', 0.05945076967592591), ('google.com', 0.07033799768518517), ('youtube.com', 0.07033799768518517), ('linkedin.com', 0.07033799768518517), ('google.com', 0.09632866898148146), ('twitter.com', 0.09632866898148146)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('google.com', 0.30407918952546287), ('linkedin.com', 0.17047207233796294), ('facebook.com', 0.11068477430555554), ('twitter.com', 0.24429189149305547), ('youtube.com', 0.17047207233796294)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('facebook.com', 0.10135972984182096), ('linkedin.com', 0.10135972984182096), ('youtube.com', 0.10135972984182096), ('google.com', 0.08523603616898147), ('twitter.com', 0.08523603616898147), ('google.com', 0.05534238715277777), ('twitter.com', 0.05534238715277777), ('google.com', 0.08143063049768516), ('youtube.com', 0.08143063049768516), ('linkedin.com', 0.08143063049768516), ('google.com', 0.08523603616898147), ('twitter.com', 0.08523603616898147)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('google.com', 0.29115832649016193), ('linkedin.com', 0.18537180628858022), ('facebook.com', 0.1161557703655478), ('twitter.com', 0.22194229056712958), ('youtube.com', 0.18537180628858022)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('facebook.com', 0.09705277549672064), ('linkedin.com', 0.09705277549672064), ('youtube.com', 0.09705277549672064), ('google.com', 0.09268590314429011), ('twitter.com', 0.09268590314429011), ('google.com', 0.0580778851827739), ('twitter.com', 0.0580778851827739), ('google.com', 0.07398076352237652), ('youtube.com', 0.07398076352237652), ('linkedin.com', 0.07398076352237652), ('google.com', 0.09268590314429011), ('twitter.com', 0.09268590314429011)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('google.com', 0.2998158867446711), ('linkedin.com', 0.1753785081662326), ('facebook.com', 0.11249485917221254), ('twitter.com', 0.23693223775065098), ('youtube.com', 0.1753785081662326)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('facebook.com', 0.09993862891489036), ('linkedin.com', 0.09993862891489036), ('youtube.com', 0.09993862891489036), ('google.com', 0.0876892540831163), ('twitter.com', 0.0876892540831163), ('google.com', 0.05624742958610627), ('twitter.com', 0.05624742958610627), ('google.com', 0.07897741258355033), ('youtube.com', 0.07897741258355033), ('linkedin.com', 0.07897741258355033), ('google.com', 0.0876892540831163), ('twitter.com', 0.0876892540831163)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('google.com', 0.29401284778550585), ('linkedin.com', 0.1820786352736746), ('facebook.com', 0.1149478345776568), ('twitter.com', 0.22688204708948803), ('youtube.com', 0.1820786352736746)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('facebook.com', 0.09800428259516862), ('linkedin.com', 0.09800428259516862), ('youtube.com', 0.09800428259516862), ('google.com', 0.0910393176368373), ('twitter.com', 0.0910393176368373), ('google.com', 0.0574739172888284), ('twitter.com', 0.0574739172888284), ('google.com', 0.07562734902982934), ('youtube.com', 0.07562734902982934), ('linkedin.com', 0.07562734902982934), ('google.com', 0.0910393176368373), ('twitter.com', 0.0910393176368373)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('google.com', 0.2979029163534824), ('linkedin.com', 0.17758688688124824), ('facebook.com', 0.11330364020589333), ('twitter.com', 0.23361966967812756), ('youtube.com', 0.17758688688124824)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('facebook.com', 0.09930097211782747), ('linkedin.com', 0.09930097211782747), ('youtube.com', 0.09930097211782747), ('google.com', 0.08879344344062412), ('twitter.com', 0.08879344344062412), ('google.com', 0.056651820102946664), ('twitter.com', 0.056651820102946664), ('google.com', 0.07787322322604252), ('youtube.com', 0.07787322322604252), ('linkedin.com', 0.07787322322604252), ('google.com', 0.08879344344062412), ('twitter.com', 0.08879344344062412)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('google.com', 0.2952951406787018), ('linkedin.com', 0.18059806604228948), ('facebook.com', 0.11440582630015335), ('twitter.com', 0.22910290093656566), ('youtube.com', 0.18059806604228948)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 574:=================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('google.com', 0.2952951406787018), ('linkedin.com', 0.18059806604228948), ('facebook.com', 0.11440582630015335), ('twitter.com', 0.22910290093656566), ('youtube.com', 0.18059806604228948)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Parameters\n",
    "ITERATIONS, a, N = 10, 0.15, 5\n",
    "\n",
    "# Inline example data: (Website, List[Linked Websites])\n",
    "data = [\n",
    "    (\"google.com\", [\"facebook.com\", \"linkedin.com\", \"youtube.com\"]),\n",
    "    (\"twitter.com\", [\"google.com\", \"youtube.com\", \"linkedin.com\"]),\n",
    "    (\"facebook.com\", [\"google.com\", \"twitter.com\"]),\n",
    "    (\"youtube.com\", [\"google.com\", \"twitter.com\"]),\n",
    "    (\"linkedin.com\", [\"google.com\", \"twitter.com\"])\n",
    "]\n",
    "\n",
    "links = sc.parallelize(data).partitionBy(8).persist()\n",
    "print(links.collect())\n",
    "\n",
    "# Initialize ranks: RDD of (Website, initial rank)\n",
    "ranks = links.mapValues(lambda _: 1.0 / N)\n",
    "\n",
    "print(links.join(ranks).collect())\n",
    "print(links.join(ranks).collect()[1])\n",
    "print(links.join(ranks).collect()[1][1])\n",
    "print(links.join(ranks).collect()[1][0])\n",
    "\n",
    "# PageRank Iterations\n",
    "for _ in range(ITERATIONS):\n",
    "    contribs = links.join(ranks).flatMap(\n",
    "        lambda site_links_rank: [(dest, site_links_rank[1][1] / len(site_links_rank[1][0])) \n",
    "                                 for dest in site_links_rank[1][0]]\n",
    "    )\n",
    "    print(contribs.collect())\n",
    "    ranks = contribs.reduceByKey(lambda x, y: x + y).mapValues(lambda total: a / N + (1 - a) * total)\n",
    "    print(ranks.collect())\n",
    "# Output final ranks\n",
    "print(ranks.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark + Minio\n",
    "\n",
    "Nous allons utiliser le stockage objet Minio pour héberger les données de notre installation Spark.\n",
    "\n",
    "MinIO est une solution de stockage objet haute performance, compatible avec l'API S3 d'AWS, permettant de gérer des données non structurées à grande échelle. Il est conçu pour des environnements cloud, hybrides ou sur site, offrant une infrastructure de stockage distribuée et évolutive.\n",
    "\n",
    "La copie de fichier peut se faire par la bibliothèque Minio Python, ou alors par le biais de l'UI Web, accessible sur http://localhost:19001. Les identifiants sont \"root\" et \"password\" (on peut les retrouver dans le fichier [docker-compose.yml](docker-compose.yml)). \n",
    "\n",
    "Le principe général de MinIO (comme pour les autres systèmes de stockage objet tels qu'AWS S3) est d'organiser les données en buckets, qui sont des conteneurs virtuels pour le stockage de fichiers ou d’objets. Chaque bucket est unique dans le système et peut contenir un nombre illimité d'objets, identifiés par des clés uniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice: Moby Dick\n",
    "\n",
    "Prérequis : le contenu du livre est présent dans le fichier `pg2701.txt` ([lien internet](https://nyu-cds.github.io/python-bigdata/files/pg2701.txt)) du TP4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-25 10:47:03--  https://nyu-cds.github.io/python-bigdata/files/pg2701.txt\n",
      "Resolving nyu-cds.github.io (nyu-cds.github.io)... 185.199.111.153, 185.199.108.153, 185.199.109.153, ...\n",
      "Connecting to nyu-cds.github.io (nyu-cds.github.io)|185.199.111.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1257296 (1.2M) [text/plain]\n",
      "Saving to: ‘pg2701.txt’\n",
      "\n",
      "pg2701.txt          100%[===================>]   1.20M  6.95MB/s    in 0.2s    \n",
      "\n",
      "2025-02-25 10:47:03 (6.95 MB/s) - ‘pg2701.txt’ saved [1257296/1257296]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://nyu-cds.github.io/python-bigdata/files/pg2701.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramètres utilisés pour stockage\n",
    "import docker\n",
    "\n",
    "minio_ip_address = \"minio\"\n",
    "\n",
    "# Décommenter dans le cas de WSL \n",
    "#network_name = \"tp4_default\"\n",
    "#container_name = \"minio\"\n",
    "#\n",
    "#client = docker.from_env()\n",
    "#container = client.containers.get(container_name)\n",
    "#\n",
    "#minio_ip_address: str = container.attrs['NetworkSettings']['Networks'][network_name]['IPAddress']\n",
    "#minio_ip_address\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", f\"http://{minio_ip_address}:9000\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"root\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"password\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minio.helpers.ObjectWriteResult at 0x735b5c06e7b0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://min.io/docs/minio/linux/developers/python/API.html\n",
    "\n",
    "from minio import Minio\n",
    "client_minio = Minio(\n",
    "    f\"{minio_ip_address}:9000\",\n",
    "    access_key=\"root\",\n",
    "    secret_key=\"password\",\n",
    "    secure=False\n",
    ")\n",
    "\n",
    "# Création du bucket tp5\n",
    "if client_minio.bucket_exists(\"tp4\") == False:\n",
    "    client_minio.make_bucket(\"tp4\")\n",
    "client_minio.fput_object(\"tp4\", \"pg2701.txt\", \"pg2701.txt\") # copie du fichier local dans le bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minio reprend le principe du stockage cloud S3 : il permet de stocker des fichiers dans des \"buckets\". Les buckets dans MinIO sont des conteneurs de stockage pour organiser et gérer des objets (fichiers) de manière structurée, similaires aux dossiers dans un système de fichiers, mais optimisés pour le stockage objet.\n",
    "Vérifiez que le fichier est bien présent dans le bucket `tp4` de Minio : [http://localhost:19001/browser/tp4/](http://localhost:19001/browser/tp4/). Vous pouvez utiliser l'utilisateur `root` et le mot de passe `password` pour vous connecter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/25 10:47:18 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "[Stage 575:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Project Gutenberg EBook of Moby Dick; or The Whale, by Herman Melville', '', 'This eBook is for the use of anyone anywhere at no cost and with', 'almost no restrictions whatsoever.  You may copy it, give it away or', 're-use it under the terms of the Project Gutenberg License included', 'with this eBook or online at www.gutenberg.org', '', '', 'Title: Moby Dick; or The Whale', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "minio_file = \"s3a://tp4/pg2701.txt\"\n",
    "# adresse du fichier dans le bucket minio\n",
    "text = sc.textFile(minio_file) \n",
    "print(text.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1\n",
    "\n",
    "1. Compter le nombre de mots total du livre.\n",
    "2. Compter le nombre d'occurrences par mot, trier par nombre décroissant (prendre les 10 premiers).\n",
    "3. Compter le nombre de mots par phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de mots: 215135\n"
     ]
    }
   ],
   "source": [
    "text = sc.textFile(minio_file) \n",
    "count_words = text.flatMap(lambda x:x.split()) \\\n",
    "    .count()\n",
    "print(f\"Nombre de mots: {count_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots les plus employés: [('the', 13765), ('of', 6587), ('and', 5951), ('a', 4533), ('to', 4510), ('in', 3879), ('that', 2693), ('his', 2415), ('I', 1724), ('with', 1692)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/25 11:22:42 ERROR TaskSchedulerImpl: Lost executor 0 on 172.26.0.2: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_667_1 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_667_6 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_408_5 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_778_2 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_667_0 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_778_6 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_577_4 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_778_3 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_667_5 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_577_5 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_667_4 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_408_0 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_778_0 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_778_7 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_408_6 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_577_3 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_667_7 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_408_1 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_667_2 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_408_3 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_577_7 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_408_4 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_577_0 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_778_4 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_778_1 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_408_2 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_577_1 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_577_6 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_577_2 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_408_7 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_778_5 !\n",
      "25/02/25 11:22:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_667_3 !\n"
     ]
    }
   ],
   "source": [
    "text = sc.textFile(minio_file) \n",
    "count_by_words = text.flatMap(lambda x: x.split()) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda x1, x2:  x1 + x2) \\\n",
    "    .sortBy(lambda x: x[1], ascending=False)\n",
    "output = count_by_words.take(10)\n",
    "print(f\"Mots les plus employés: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = sc.textFile(minio_file)\n",
    "phrases = text.flatMap(lambda ligne: ligne.split(\".\"))\n",
    "mots_par_phrase = phrases.map(lambda phrase: len(phrase.split()))\n",
    "total_mots = mots_par_phrase.reduce(lambda a, b: a + b)\n",
    "nombre_phrases = mots_par_phrase.count()\n",
    "moyenne_mots_par_phrase = total_mots / nombre_phrases\n",
    "\n",
    "print(f\"Moyenne de mots par phrase : {moyenne_mots_par_phrase}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Télécharger un autre livre (en trouver un sur https://www.gutenberg.org/browse/scores/top par exemple, télécharger au format \"Plain Text UTF-8\"), et lancer les jobs dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.gutenberg.org/cache/epub/345/pg345.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_minio.fput_object(\"tp4\", \"pg345.txt\", \"pg345.txt\") # copie du fichier local dans le bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_file = \"s3a://tp4/pg345.txt\"\n",
    "text = sc.textFile(minio_file) \n",
    "count_words = text.flatMap(lambda x:x.split()) \\\n",
    "    .count()\n",
    "print(f\"Nombre de mots: {count_words}\")\n",
    "text = sc.textFile(minio_file) \n",
    "count_by_words = text.flatMap(lambda x: x.split()) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda x1, x2:  x1 + x2) \\\n",
    "    .sortBy(lambda x: x[1], ascending=False)\n",
    "output = count_by_words.take(10)\n",
    "print(f\"Mots les plus employés: {output}\")\n",
    "text = sc.textFile(minio_file)\n",
    "phrases = text.flatMap(lambda ligne: ligne.split(\".\"))\n",
    "mots_par_phrase = phrases.map(lambda phrase: len(phrase.split()))\n",
    "total_mots = mots_par_phrase.reduce(lambda a, b: a + b)\n",
    "nombre_phrases = mots_par_phrase.count()\n",
    "moyenne_mots_par_phrase = total_mots / nombre_phrases\n",
    "\n",
    "print(f\"Moyenne de mots par phrase : {moyenne_mots_par_phrase}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2 : JSON\n",
    "\n",
    "Charger le fichier `countries.json` ((adresse)[http://api.worldbank.org/v2/countries?per_page=304&format=json]), et chargez le dans Minio comme fait précédemment. \n",
    "\n",
    "Calculez ensuite le nombre de pays par niveau de revenu à l'aide d'un job Pyspark.\n",
    "\n",
    "Vous pouvez vous aider des lignes suivantes en premier lieu (afin de se concentrer directement sur le tableau des pays, contenu dans le deuxième élément du tableau racine) :\n",
    "```python\n",
    "rdd = sc.textFile(f\"s3a://tp4/countries.json\") # chargement du fichier countries.json\n",
    "mapped_rdd = rdd.map(lambda f: json.loads(f)) # chargement du fichier json dans un dictionnaire\n",
    "country_rdd = mapped_rdd.flatMap(lambda x: x[1]) # on récupère le tableau des pays\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O countries.json  \"http://api.worldbank.org/v2/countries?per_page=304&format=json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_file = \"countries.json\"\n",
    "client_minio.fput_object(\"tp4\", minio_file, minio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "rdd = sc.textFile(f\"s3a://tp4/{minio_file}\")\n",
    "mapped_rdd = rdd.map(lambda line: json.loads(line))\n",
    "country_rdd = mapped_rdd.flatMap(lambda x: x[1])\n",
    "print(country_rdd.take(10))\n",
    "income_counts = country_rdd.map(lambda country: (country['incomeLevel']['value'], 1)) \\\n",
    "                           .reduceByKey(lambda a, b: a + b)\n",
    "# Collect and print results\n",
    "print(\"Number of countries by income level:\")\n",
    "for income_level, count in income_counts.collect():\n",
    "    print(f\"{income_level}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 3\n",
    "\n",
    "Réalisez trois requêtes non triviales sur les fichiers json des [pokemon](https://github.com/fanzeyi/pokemon.json).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
