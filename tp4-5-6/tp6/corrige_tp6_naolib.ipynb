{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correction Naolib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configuration Mac : utilisation de notebook Jupyter\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setAppName('Naolib') \\\n",
    "    .setMaster('spark://spark:7077') \\\n",
    "    .set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .set(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "# Créer un SQLContext pour les opérations SQL\n",
    "sql_context = SQLContext(sc)\n",
    "\n",
    "minio_ip_address = \"minio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", f\"http://{minio_ip_address}:9000\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"root\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"password\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "from minio import Minio\n",
    "client_minio = Minio(\n",
    "    f\"{minio_ip_address}:9000\",\n",
    "    access_key=\"root\",\n",
    "    secret_key=\"password\",\n",
    "    secure=False\n",
    ")\n",
    "\n",
    "# Création du bucket tp6\n",
    "if client_minio.bucket_exists(\"tp6\") == False:\n",
    "    client_minio.make_bucket(\"tp6\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lag, regexp_extract, lit, unix_timestamp\n",
    "from pyspark.sql.window import Window\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# URL de l'API\n",
    "API_URL = \"https://open.tan.fr/ewp/tempsattentelieu.json/CTRE/20\"\n",
    "\n",
    "# Fonction pour récupérer les données depuis l'API\n",
    "def fetch_api_data():\n",
    "    try:\n",
    "        response = requests.get(API_URL)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"Erreur HTTP : {response.status_code}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'accès à l'API : {e}\")\n",
    "        return []\n",
    "\n",
    "# Récupérer les données depuis l'API\n",
    "data = fetch_api_data()\n",
    "\n",
    "if data:\n",
    "    # Charger les données JSON dans un DataFrame PySpark\n",
    "    df = sql_context.read.json(sc.parallelize([json.dumps(data)]))\n",
    "\n",
    "    # Extraire les colonnes importantes et nettoyer les données\n",
    "    df_cleaned = df \\\n",
    "        .withColumn(\"numLigne\", col(\"ligne.numLigne\")) \\\n",
    "        .withColumn(\"terminus\", col(\"terminus\")) \\\n",
    "        .withColumn(\"codeArret\", col(\"arret.codeArret\")) \\\n",
    "        .withColumn(\"tempsMinutes\", \n",
    "                    when(col(\"temps\") == \"proche\", lit(0))\n",
    "                    .otherwise(regexp_extract(col(\"temps\"), r\"(\\d+)\", 1).cast(\"int\"))) \\\n",
    "        .filter(col(\"tempsMinutes\").isNotNull()) \\\n",
    "        .select(\"numLigne\", \"terminus\", \"codeArret\", \"tempsMinutes\")\n",
    "\n",
    "    # Ajouter une colonne avec les temps précédents pour calculer le délai\n",
    "    window_spec = Window.partitionBy(\"numLigne\", \"codeArret\").orderBy(\"tempsMinutes\") # demander explications à Guillaume\n",
    "    df_delai = df_cleaned \\\n",
    "        .withColumn(\"tempsPrecedent\", lag(\"tempsMinutes\").over(window_spec)) \\\n",
    "        .withColumn(\"delaiEntreBus\", col(\"tempsMinutes\") - col(\"tempsPrecedent\"))\n",
    "\n",
    "    # Afficher les résultats\n",
    "    df_delai.select(\"numLigne\", \"terminus\", \"codeArret\", \"tempsMinutes\", \"tempsPrecedent\", \"delaiEntreBus\").show()\n",
    "\n",
    "else:\n",
    "    print(\"Aucune donnée disponible depuis l'API.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming \n",
    "\n",
    "Dans ce cas, nous synchronisons les résultats des requêtes dans le bucket Minio disponible à l'adresse suivante : [tp6](http://localhost:19001/tp6), dans le dossier `naolib`.\n",
    "Pour cela, il est nécessaire de lancer le notebook [naolib_correction_insert.ipynb](naolib_correction_insert.ipynb) en parallèle pour insérer les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, regexp_extract, lit, window, min, max, current_timestamp\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define schema\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "schema = StructType([\n",
    "    StructField(\"sens\", StringType()),\n",
    "    StructField(\"terminus\", StringType()),\n",
    "    StructField(\"infotrafic\", StringType()),\n",
    "    StructField(\"temps\", StringType()),\n",
    "    StructField(\"dernierDepart\", StringType()),\n",
    "    StructField(\"tempsReel\", StringType()),\n",
    "    StructField(\"ligne\", StructType([\n",
    "        StructField(\"numLigne\", StringType()),\n",
    "        StructField(\"typeLigne\", StringType())\n",
    "    ])),\n",
    "    StructField(\"arret\", StructType([\n",
    "        StructField(\"codeArret\", StringType())\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Read streaming data from MinIO\n",
    "input_path = \"s3a://tp6/naolib\"\n",
    "df = sql_context.readStream.schema(schema).json(input_path)\n",
    "\n",
    "# Add a timestamp column and clean data\n",
    "df_cleaned = df.withColumn(\"numLigne\", col(\"ligne.numLigne\")) \\\n",
    "    .withColumn(\"codeArret\", col(\"arret.codeArret\")) \\\n",
    "    .withColumn(\"tempsMinutes\", when(col(\"temps\") == \"proche\", lit(0))\n",
    "                .otherwise(regexp_extract(col(\"temps\"), r\"(\\d+)\", 1).cast(\"int\"))) \\\n",
    "    .withColumn(\"event_time\", current_timestamp())  # Add ingestion time as timestamp\n",
    "\n",
    "# Add watermark and calculate delays\n",
    "df_with_watermark = df_cleaned.withWatermark(\"event_time\", \"10 minutes\")\n",
    "\n",
    "windowed_df = df_with_watermark.groupBy(\n",
    "    window(col(\"event_time\"), \"10 minutes\"),\n",
    "    col(\"numLigne\"),\n",
    "    col(\"codeArret\")\n",
    ").agg(\n",
    "    min(\"tempsMinutes\").alias(\"tempsPrecedent\"),\n",
    "    max(\"tempsMinutes\").alias(\"tempsCurrent\")\n",
    ").withColumn(\n",
    "    \"delaiEntreBus\", col(\"tempsCurrent\") - col(\"tempsPrecedent\")\n",
    ")\n",
    "\n",
    "# Write results to the console\n",
    "query = windowed_df.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
