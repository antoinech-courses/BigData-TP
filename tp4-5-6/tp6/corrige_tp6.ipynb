{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correction TP6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configuration Mac : utilisation de notebook Jupyter\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setAppName('SparkApp') \\\n",
    "    .setMaster('spark://spark:7077') \\\n",
    "    .set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .set(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "# Créer un SQLContext pour les opérations SQL\n",
    "sql_context = SQLContext(sc)\n",
    "\n",
    "minio_ip_address = \"minio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", f\"http://{minio_ip_address}:9000\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"root\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"password\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "from minio import Minio\n",
    "client_minio = Minio(\n",
    "    f\"{minio_ip_address}:9000\",\n",
    "    access_key=\"root\",\n",
    "    secret_key=\"password\",\n",
    "    secure=False\n",
    ")\n",
    "\n",
    "# Création du bucket tp6\n",
    "if client_minio.bucket_exists(\"tp6\") == False:\n",
    "    client_minio.make_bucket(\"tp6\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_minio.fput_object(\"tp6\", \"160109-histoire.txt\", \"allData/160109-histoire.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données\n",
    "file_path = \"s3a://tp6/160109-histoire.txt\"\n",
    "#lines = sql_context.read.text(file_path).collect()\n",
    "#print(lines)\n",
    "lines = sql_context.read.text(file_path).rdd.map(lambda r: r[0])\n",
    "print(lines.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation : Diviser les lignes en mots\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Agrégation : Compter les mots\n",
    "word_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Trier par fréquence décroissante\n",
    "sorted_word_counts = word_counts.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Table des mots et leurs comptes :\")\n",
    "for word, count in sorted_word_counts.take(10):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Filtrer les mots avec une longueur >= 6\n",
    "filtered_word_counts = sorted_word_counts.filter(lambda x: len(x[0]) >= 6)\n",
    "\n",
    "# Afficher les résultats filtrés\n",
    "print(\"Mots de longueur >= 6 et leurs comptes :\")\n",
    "for word, count in filtered_word_counts.take(10):\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2 : Stream "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split, col\n",
    "# Étape 2 : Définir la source (le répertoire \"data\" contenant les fichiers texte)\n",
    "input_dir = \"s3a://tp6/allData\"  # Répertoire de surveillance\n",
    "checkpoint_dir = \"checkpoint\"  # Répertoire pour la reprise\n",
    "\n",
    "lines = sql_context.readStream \\\n",
    "    .format(\"text\") \\\n",
    "    .load(input_dir)\n",
    "\n",
    "# Étape 3 : Transformer les données\n",
    "# Diviser les lignes en mots\n",
    "words = lines.select(explode(split(col(\"value\"), \" \")).alias(\"word\"))\n",
    "\n",
    "# Compter les occurrences des mots\n",
    "word_counts = words.groupBy(\"word\").count()\n",
    "\n",
    "# Sort word counts in descending order\n",
    "sorted_word_counts = word_counts.orderBy(col(\"count\").desc())\n",
    "\n",
    "# Étape 4 : Définir la sortie\n",
    "# Sortie sur la console avec affichage de 20 lignes par micro-batch\n",
    "query = sorted_word_counts.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"numRows\", 20) \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()\n",
    "\n",
    "# Étape 5 : Démarrer la requête\n",
    "query.awaitTermination()\n",
    "\n",
    "# https://stackoverflow.com/questions/61463554/structured-streaming-output-is-not-showing-on-jupyter-notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, col, regexp_replace, length\n",
    "\n",
    "\n",
    "# Définir les chemins\n",
    "input_dir = \"data\"  # Répertoire surveillé\n",
    "checkpoint_dir = \"checkpoint\"  # Répertoire de checkpoint\n",
    "\n",
    "# Définir les mots à exclure\n",
    "excluded_words = {\"quelques\", \"toujours\", \"ceci\", \"cela\", \"mais\", \"donc\", \"or\", \"ni\", \"car\"}\n",
    "\n",
    "input_dir = \"s3a://tp6/allData\"  # Répertoire de surveillance\n",
    "\n",
    "lines = sql_context.readStream \\\n",
    "    .format(\"text\") \\\n",
    "    .load(input_dir)\n",
    "\n",
    "\n",
    "# Transformation : nettoyer les mots et filtrer\n",
    "words = lines.select(\n",
    "    explode(\n",
    "        split(\n",
    "            regexp_replace(col(\"value\"), r\"[^\\w\\s]\", \"\"),  # Supprimer la ponctuation\n",
    "            \" \"\n",
    "        )\n",
    "    ).alias(\"word\")\n",
    ")\n",
    "\n",
    "# Filtrer les mots\n",
    "filtered_words = words.filter(\n",
    "    (length(col(\"word\")) > 7) &   # Garde les mots avec plus de 7 lettres\n",
    "    (~col(\"word\").isin(*excluded_words)) &  # Exclut les mots dans excludedWords\n",
    "    (col(\"word\") != \"\")  # Exclut les mots vides\n",
    ")\n",
    "\n",
    "# Compter les occurrences des mots filtrés\n",
    "word_counts = filtered_words.groupBy(\"word\").count()\n",
    "\n",
    "# Trier les mots par fréquence décroissante\n",
    "sorted_word_counts = word_counts.orderBy(col(\"count\").desc())\n",
    "\n",
    "# Écrire le résultat sur la console avec les 20 mots les plus fréquents\n",
    "query = sorted_word_counts.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"numRows\", 20) \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()\n",
    "\n",
    "# Attendre la fin de l'exécution\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
