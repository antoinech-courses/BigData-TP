{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scripts du cours Spark SQL\n",
    "\n",
    "Attention, si vous utilisez la version comprenant Jupyter (utilisation de docker-compose-jupyter.yml pour les Mac par exemple) , utilisez la configuration suivante (cellule désactivée) de Spark à la place de la suivante.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5d480b31-4cfb-4458-941c-7389e65eb942;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.3.4!hadoop-aws.jar (427ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.12.262!aws-java-sdk-bundle.jar (137319ms)\n",
      "downloading https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.0.7.Final/wildfly-openssl-1.0.7.Final.jar ...\n",
      "\t[SUCCESSFUL ] org.wildfly.openssl#wildfly-openssl;1.0.7.Final!wildfly-openssl.jar (270ms)\n",
      ":: resolution report :: resolve 6099ms :: artifacts dl 138092ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5d480b31-4cfb-4458-941c-7389e65eb942\n",
      "\tconfs: [default]\n",
      "\t3 artifacts copied, 0 already retrieved (275421kB/9244ms)\n",
      "25/03/04 08:58:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/opt/conda/lib/python3.12/site-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### Configuration Mac : utilisation de notebook Jupyter\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setAppName('SparkApp') \\\n",
    "    .setMaster('spark://spark:7077') \\\n",
    "    .set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") # utilisé pour le stockage \n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "# Créer un SQLContext pour les opérations SQL\n",
    "sql_context = SQLContext(sc)\n",
    "\n",
    "minio_ip_address = \"minio\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames vs Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|avg(age)|\n",
      "+------+--------+\n",
      "|Brooke|    22.5|\n",
      "| Jules|    30.0|\n",
      "|    TD|    35.0|\n",
      "| Denny|    31.0|\n",
      "+------+--------+\n",
      "\n",
      "+------+-------+\n",
      "|  name|avg_age|\n",
      "+------+-------+\n",
      "|Brooke|   22.5|\n",
      "| Jules|   30.0|\n",
      "|    TD|   35.0|\n",
      "| Denny|   31.0|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Create DataFrame\n",
    "data = [(\"Brooke\", 20), (\"Brooke\", 25), (\"Denny\", 31), (\"Jules\", 30), (\"TD\", 35)]\n",
    "df = sql_context.createDataFrame(data, [\"name\", \"age\"])\n",
    "# Calculate average age by name using DataFrame API\n",
    "avgDF = df.groupBy(\"name\").agg(avg(\"age\"))\n",
    "avgDF.show()# Create a temporary view to use SQL\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "# Calculate average age by name using SQL\n",
    "sqlDF = sql_context.sql(\"\"\"\n",
    "SELECT name, avg(age) as avg_age\n",
    "FROM people\n",
    "GROUP BY name\n",
    "\"\"\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colonnes et expressions sur les colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name', 'age']\n",
      "Column<'name'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row((age + 1)=21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row((age + 1)=21)\n",
      "Row((age + 1)=21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "print(df.columns)\n",
    "\n",
    "print(df[\"name\"])\n",
    "\n",
    "print(df.select(col(\"age\") + 1).first())\n",
    "\n",
    "print(df.select(df.age + 1).first())\n",
    "\n",
    "print(df.select(expr(\"age + 1\")).first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(name='Brooke', age=20)\n",
      "[Row(name='Jules', age=30), Row(name='TD', age=35)]\n",
      "<Row('Brooke', 20)>\n",
      "Brooke\n",
      "20\n",
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Brooke| 20|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "first_row = df.first()\n",
    "print(first_row)  # Output: Row(name='Brooke', age=20)\n",
    "\n",
    "tail_rows = df.tail(2)\n",
    "print(tail_rows)  # Output: [Row(name='Jules', age=30), Row(name='TD', age=35)]\n",
    "\n",
    "row = Row(\"Brooke\", 20)\n",
    "print(row)  # Output: Row(Brooke=20)\n",
    "\n",
    "print(row[0])  # Output: Brooke\n",
    "print(row[1])  # Output: 20\n",
    "\n",
    "# Convert sequence to DataFrame\n",
    "seq_df = sql_context.createDataFrame([(\"Brooke\", 20)], [\"name\", \"age\"])\n",
    "seq_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection et sélection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Brooke|\n",
      "| Jules|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply transformations\n",
    "result = (\n",
    "    df.select(\"name\")             # Projection: Select only the \"name\" column\n",
    "      .where(\"age <= 30\")         # Selection: Filter rows where \"age\" is less than or equal to 30\n",
    "      .distinct()                 # Eliminate duplicates\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renommage, ajout et suppression de colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|   nom|age|\n",
      "+------+---+\n",
      "|Brooke| 20|\n",
      "+------+---+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|  name|dizaine|\n",
      "+------+-------+\n",
      "|Brooke|      2|\n",
      "+------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed(\"name\", \"nom\").show(1)\n",
    "\n",
    "from pyspark.sql.functions import floor, col\n",
    "df.withColumn(\"dizaine\", floor(col(\"age\") / 10))  \\\n",
    "    .drop(\"age\") \\\n",
    "    .show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrégats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|  name|max_age|\n",
      "+------+-------+\n",
      "|Brooke|     25|\n",
      "| Jules|     30|\n",
      "|    TD|     35|\n",
      "| Denny|     31|\n",
      "+------+-------+\n",
      "\n",
      "+--------+--------+--------+\n",
      "|min(age)|max(age)|avg(age)|\n",
      "+--------+--------+--------+\n",
      "|      20|      35|    28.2|\n",
      "+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "result = (\n",
    "    df.groupBy(\"name\")       # Group by \"name\"\n",
    "      .agg(max(\"age\").alias(\"max_age\"))  # Calculate max \"age\"\n",
    ").show()\n",
    "from pyspark.sql.functions import min, max, avg\n",
    "df.select(min(\"age\"), max(\"age\"), avg(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|   nom|age|\n",
      "+------+---+\n",
      "|Brooke| 20|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Define schema with StructType and StructField\n",
    "schema = StructType([\n",
    "    StructField(\"nom\", StringType(), nullable=False),\n",
    "    StructField(\"age\", IntegerType(), nullable=False)\n",
    "])\n",
    "\n",
    "# Create data\n",
    "data = [Row(\"Brooke\", 20)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = sql_context.createDataFrame(data, schema)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample CSV file created!\n"
     ]
    }
   ],
   "source": [
    "# Create a sample CSV file\n",
    "import os\n",
    "\n",
    "csv_content = \"\"\"name,age\n",
    "Brooke,20\n",
    "Jules,30\n",
    "Cameron,35\"\"\"\n",
    "\n",
    "with open(\"sample.csv\", \"w\") as file:\n",
    "    file.write(csv_content)\n",
    "\n",
    "print(\"Sample CSV file created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minio.helpers.ObjectWriteResult at 0x7fea50e0e690>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", f\"http://{minio_ip_address}:9000\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"root\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"password\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "from minio import Minio\n",
    "client_minio = Minio(\n",
    "    f\"{minio_ip_address}:9000\",\n",
    "    access_key=\"root\",\n",
    "    secret_key=\"password\",\n",
    "    secure=False\n",
    ")\n",
    "\n",
    "# Création du bucket tp6\n",
    "if client_minio.bucket_exists(\"tp6\") == False:\n",
    "    client_minio.make_bucket(\"tp6\")\n",
    "client_minio.fput_object(\"tp6\", \"sample.csv\", \"sample.csv\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/04 09:02:53 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "| Brooke| 20|\n",
      "|  Jules| 30|\n",
      "|Cameron| 35|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read CSV file\n",
    "df = (\n",
    "    sql_context.read\n",
    "    .format(\"csv\")                  # Specify format as CSV\n",
    "    .option(\"inferSchema\", \"true\")  # Infer the schema automatically\n",
    "    .option(\"header\", \"true\")       # The file contains a header\n",
    "    .option(\"mode\", \"FAILFAST\")     # Fail if there are any errors\n",
    "    .option(\"nullValue\", \"\")        # Treat empty strings as null values\n",
    "    .load(\"s3a://tp6/sample.csv\")                 # Path to the CSV file\n",
    ")\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+--------------+\n",
      "|  name|age|age_in_5_years|\n",
      "+------+---+--------------+\n",
      "|Brooke| 20|            25|\n",
      "+------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Example: Add a new column and filter rows\n",
    "df_transformed = (\n",
    "    df.withColumn(\"age_in_5_years\", col(\"age\") + 5)  # Add 5 to the age\n",
    "      .filter(col(\"age\") < 30)                      # Keep rows where age is less than 30\n",
    ")\n",
    "\n",
    "# Show the transformed DataFrame\n",
    "df_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/04 09:03:17 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(df_transformed.coalesce(1).write           # execute on one task\n",
    "    .format(\"csv\")                          # output format \n",
    "    .option(\"header\", \"true\")               # header present\n",
    "    .mode(\"overwrite\")                      # overwrite if exist\n",
    "    .save(\"s3a://tp6/transformed_sample\"))  # directory name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez aller vérifier sur [Minio](http://localhost:19001/browser/tp6/) (pour rappel utilisateur et mot de passe : root / password ) dans le bucket TP6 que le fichier a bien été généré (dans le dossier `transformed_sample`). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
